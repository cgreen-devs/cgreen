Cgreen - Unit Tests for C and C++
===============================

Cgreen Quickstart Guide
-----------------------

What is Cgreen?
~~~~~~~~~~~~~~~

*Cgreen* is a unit tester for the C and C++ software developer, a test
automation and software quality assurance tool for development
teams. The tool is completely open source published under the
http://www.gnu.org/licenses/lgpl.html[LGPL]

Unit testing is a development practice popularised by the agile
development community.  It is characterised by writing many small
tests alongside the normal code. Often the tests are written before
the code they are testing, in a tight test-code-refactor loop.  Done
this way, the practice is known as Test Driven Development. *Cgreen*
was designed to support this style of development.

Unit tests are written in the same language as the code, in our case
C or C++. This avoids the mental overhead of constantly switching language,
and also allows you to use any application code in your tests.

Here are some of its features:

- Fully composable test suites
- 'setup()' and 'teardown()' for tests and test suites
- Each test runs in it's own process
- An isolated test can be run in a single process for debugging
- Ability to mock functions
- The reporting mechanism can be easily extended
- Automatic discovery of tests
        
*Cgreen* was primarily developed to support C programming, but there
is also support for C++.


Cgreen - Vanilla or Chocolate?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Test driven development (TDD) catched on when the JUnit framework for
Java spread to other langauges, giving us a family of xUnit
tools. *Cgreen* was born in this wave and have many similarities to
the xUnit family.

But TDD evolved over time and modern thinking and practice is more
along the lines of BDD, an abbreviation of Behaviour Driven
Development, made popular by people like Dan North and frameworks like
JBehave, RSpec, Cucumber and Jasmine.

*Cgreen* follows this trend and has evolved to embrace BDD-style
testing. Although the fundamental mechanisms in TDD and BDD are
much the same, the shift in focus by changing wording from 'tests' to
'behaviour specifications' is significant.

This document will present TDD-style Cgreen first and then go on to
describe the minor differences that will allow you to think, drive and
test in BDD fashion.

Installing Cgreen
~~~~~~~~~~~~~~~~~

There are two ways to install *Cgreen* in your system.

Installing a package
^^^^^^^^^^^^^^^^^^^^
      
The first way is to use the RPM or DEB package provided by the *Cgreen*
Team. You can fetch it from http://cgreen.sourceforge.net[Cgreen
website project]. Download and install it using the normal procedures
for your system.

Installing from source
^^^^^^^^^^^^^^^^^^^^^^

The second way is indicated for developers and advanced
users. Basically this consists of fetching the sources of the project
and compiling them. To do this you need the http://www.cmake.org[CMake]
build system.

When you have the CMake tool, the steps are:

-----------------------------------------
tar -zxpvf cgreen.tar.gz
mkdir cgreen-build
cd cgreen-build
cmake ../cgreen
make
make test    
make install
-----------------------------------------

(On cygwin you need to install before the tests can be run.)

As you can see, we create a separate build directory before going
there and building. This is called an 'out of source build'. It
compiles *Cgreen* from outside the sources directory. This helps the
overall file organization and enables multi-target builds from the
same sources by leaving the complete source tree untouched.

Is possible to use the +Makefile+. This file is used to compile and
test *Cgreen* without the need of CMake tool. However it does not
contain the rules to install on your system.

Both methods will create a library (on unix called +libcgreen.so+)
which can be used in conjunction with the +cgreen.h+ header file to
compile test code. The created library is installed in the system, by
default in the +/usr/local/lib/+.

We'll first write a test to confirm everything is working. Let's start
with a simple test module with no tests, called +first_test.c+...

[source,c]
---------------------------------------
#include <cgreen/cgreen.h>

int main(int argc, char **argv) {
  TestSuite *suite = create_test_suite();
  return run_test_suite(suite, create_text_reporter());
}
---------------------------------------

This is a very unexciting test. It just creates an empty test suite
and runs it.  It's usually easier to proceed in small steps, though,
and this is the smallest one I could think of. The only complication
is the +cgreen.h+ header file.  Here I am assuming we have the
*Cgreen* folder in the include search path to ensure compilation
works, otherwise you'll need to add that in the compilation command.

Building this test is, of course, trivial...

-----------------------------
gcc -c first_test.c
gcc first_test.o -lcgreen -o first_test
./first_test
-----------------------------
          
Invoking the executable should give...

-----------------------------
Running "main" (0 tests)...
Completed "main": 0 passes, 0 failures, 0 exceptions.
-----------------------------

All of the above rather assumes you are working in a Unix like
environment, probably with 'gcc'. The code is pretty much standard
C99, so any C compiler should work.  *Cgreen* should compile on all
systems that support the +sys/msg.h+ messaging library.  This has been
tested on Linux, MacOSX and Cygwin so far, but not Windows.

So far we have tested compilation, and that the test suite actually runs.
Let's add a meaningless test or two so that you can see how it runs...

[source,c]
-----------------------------
#include <cgreen/cgreen.h>

Ensure(this_test_should_pass) {
    assert_that(1 == 1);
}

Ensure(this_test_should_fail) {
    assert_that(0, is_equal_to(1));
}

int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_test(suite, this_test_should_pass);
    add_test(suite, this_test_should_fail);
    return run_test_suite(suite, create_text_reporter());
}
-----------------------------

A test is denoted by the macro *Ensure*. You can think of a test as
having a 'void (void)' signature. You add the test to your suite using
'add_test()'.

On compiling and running, we now get the output...

-----------------------------
Running "main" (2 tests)...
first_test.c:8: Test Failure: -> this_test_should_fail
    Expected [0] to [equal] [1]
Completed "main": 1 pass, 1 failure, 0 exceptions.
-----------------------------

The 'TextReporter', created by the 'create_text_reporter()' call, is
the simplest way to output the test results. It just streams the
failures as text.

Of course "0" would never equal "1", but this shows how *Cgreen*
presents the expression that you want to assert. We can also see a
handy short hand form for boolean expressions ('assert_that(1 == 1);').


Five minutes doing TDD with Cgreen
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a more realistic example we need something to test. We'll pretend
that we are writing a function to split the words of a sentence in
place. It does this by replacing any spaces with string terminators
and returns the number of conversions plus one.  Here is an example of
what we have in mind...

[source,c]
-------------------------------
char *sentence = strdup("Just the first test");
word_count = split_words(sentence);
-------------------------------

'sentence' should now point at "Just\0the\0first\0test". Not an
obviously useful function, but we'll be using it for something more
practical later.

This time around we'll add a little more structure to our
tests. Rather than having the test as a stand alone program, we'll
separate the runner from the test cases.  That way, multiple test
suites of test cases can be included in the 'main()' runner file.
This makes it less work to add more tests.

Here is the, so far empty, test case in +words_test.c+...

[source,c]
-------------------------------
#include <cgreen/cgreen.h>

TestSuite *words_tests() {
TestSuite *suite = create_test_suite();
  return suite;
}
-------------------------------

Here is the +all_tests.c+ test runner...

[source,c]
-------------------------------
#include <cgreen/cgreen.h>

TestSuite *words_tests();

int main(int argc, char **argv) {
  TestSuite *suite = create_test_suite();
  add_suite(suite, words_tests());
  if (argc > 1) {
    return run_single_test(suite, argv[1], create_text_reporter());
  }
  return run_test_suite(suite, create_text_reporter());
}
-------------------------------

*Cgreen* has two ways of running tests. The default is to run all
tests in their own protected processes. This is what happens if you
invoke 'run_test_suite()'. All tests are then completely independent
since they run in separate processes, preventing a single run-away
test from bringing the whole program down with it. It also ensures
that one test cannot leave any state to the next, thus forcing you to
setup the prerequisites for each test correctly and clearly.

But if you want to debug any of your tests the constant 'fork()ing'
can make that difficult or impossible.  To make debugging simpler,
*Cgreen* does not fork() when only a single test is run by name with
the function 'run_single_test()'. And if you want to debug, you can
obviously set a breakpoint at that test (but note that its actual name
might have been mangled) . But since *Cgreen* does some book-keeping
before actually getting to the test, a better function is the one
simply called 'run()'.

Building this scaffolding...

-------------------------------
gcc -c words_test.c
gcc -c all_tests.c
gcc words_test.o all_tests.o -lcgreen -o all_tests
-------------------------------

...and executing the result gives the familiar...

-------------------------------
Running "main" (0 tests)...
Completed "main": 0 passes, 0 failures, 0 exceptions.
-------------------------------

All this scaffolding is pure overhead, but from now on adding tests
will be a lot easier.

Here is a first test of 'split_words()' in +words_test.c+...

[source,c]
-------------------------------
#include <cgreen/cgreen.h>;
#include "words.h";
#include <string.h>;

Ensure(word_count_returned_from_split) {
  char *sentence = strdup("Birds of a feather");
  int word_count = split_words(sentence);
  assert_that(word_count, is_equal_to(4));
  free(sentence);
}

TestSuite *words_tests() {
  TestSuite *suite = create_test_suite();
  add_test(suite, word_count_returned_from_split);
  return suite;
}
-------------------------------

The 'assert_that()' macro takes two parameters, the value to assert
and a constraint. The constraints comes in various forms. In this case
we use the probably most common, 'is_equal_to()'. With the default
'TextReporter' the message is sent to 'STDOUT'.

To get this to compile we need to create the +words.h+ header file...

[source,c]
-------------------------------
int split_words(char *sentence);
-------------------------------

...and to get the code to link we need a stub function in +words.c+...

[source,c]
-------------------------------
int split_words(char *sentence) {
  return 0;
}
-------------------------------

A full build later...

-------------------------------
gcc -c all_tests.c
gcc -c words_test.c
gcc -c words.c
gcc all_tests.o words_test.o words.o -lcgreen -o all_tests
./all_tests
-------------------------------

...and we get the more useful response...

-------------------------------
Running "main" (1 tests)...
words_test.c:10: Failure: -> words_tests -> word_count_returned_from_split 
	Expected [word_count] to [equal] [4]
		actual value:	[0]
		expected value:	[4]
Completed "main": 0 passes, 1 failure, 0 exceptions.
-------------------------------

The breadcrumb trail following the "Failure" text is the nesting of
the tests. It goes from the test suites, which can be nested in each
other, through the test function, and finally to the message from the
assertion. In the language of *Cgreen*, a "failure" is a mismatched
assertion, an "exception" occurs when a test fails to complete for any
reason.

We could get this to pass just by returning the value 4. Doing TDD in
really small steps, you would actually do this, but frankly this
example is too simple. Instead we'll go straight to the core of the
implementation...

[source,c]
--------------------------------
#include <string.h>;

int split_words(char *sentence) {
  int i, count = 1;
  for (i = 0; i < strlen(sentence); i++) {
    if (sentence[i] == ' ') {
      count++;
    }
  }
  return count;
}
---------------------------------

There is a hidden problem here, but our tests still passed so we'll
pretend we didn't notice.

[source,c]
---------------------------------
Running "main" (1 tests)...
Completed "main": 1 pass, 0 failures, 0 exceptions.
---------------------------------

Time to add another test. We want to confirm that the string is broken
into separate words...

[source,c]
---------------------------------
#include <cgreen/cgreen.h>
#include "words.h"
#include <string.h>;

Ensure(word_count_returned_from_split) { ... }

Ensure(spaces_should_be_converted_to_zeroes) {
  char *sentence = strdup("Birds of a feather");
  split_words(sentence);
  int comparison = memcmp("Birds\0of\0a\0feather", sentence, strlen(sentence));
  assert_that(comparison, is_equal_to(0));
  free(sentence); 
}

TestSuite *words_tests() {
  TestSuite *suite = create_test_suite();
  add_test(suite, word_count_returned_from_split);
  add_test(suite, spaces_should_be_converted_to_zeroes);
  return suite;
}
----------------------------------

Sure enough, we get a failure...

----------------------------------
Running "main" (2 tests)...
words_test.c:18: Failure: -> words_tests -> spaces_should_be_converted_to_zeroes 
	Expected [comparison] to [equal] [0]
		actual value:	[-32]
		expected value:	[0]
Completed "main": 1 pass, 1 failure, 0 exceptions.
----------------------------------

Not surprising given that we haven't written the code yet.

The fix...

[source,c]
----------------------------------
int split_words(char *sentence) {
  int i, count = 1;
  for (i = 0; i < strlen(sentence); i++) {
    if (sentence[i] == ' ') {
      sentence[i] = '\0';
      count++;
    }
  }
  return count;
}
----------------------------------

...reveals our previous hack...

----------------------------------
Running "main" (2 tests)...
words_test.c:10: Failure: -> words_tests -> word_count_returned_from_split 
	Expected [word_count] to [equal] [4]
		actual value:	[2]
		expected value:	[4]
Completed "main": 1 pass, 1 failure, 0 exceptions.
----------------------------------

Our earlier test now fails, because we have affected the 'strlen()'
call in our loop.  Moving the length calculation out of the loop...

[source,c]
----------------------------------
int split_words(char *sentence) {
  int i, count = 1, length = strlen(sentence);
  for (i = 0; i < length; i++) {
    ...
  }
  return count;
}
----------------------------------

...restores order...
		  
----------------------------------
Running "main" (2 tests)...
Completed "main": 2 passes, 0 failures, 0 exceptions.
----------------------------------

It's nice to keep the code under control while we are actually writing
it, rather than debugging later when things are more complicated.

That was pretty straight forward. Let's do something more interesting.

What are mock functions?
~~~~~~~~~~~~~~~~~~~~~~~~

The next example is more realistic. Still in our +words.h+ file, we
want to write a function that invokes a callback on each word in a
sentence. Something like...

[source,c]
----------------------------------
void act_on_word(const char *word, void *memo) { ... }
words("This is a sentence", &act_on_word, &memo);
----------------------------------

Here the 'memo' pointer is just some accumulated data that the
'act_on_word()' callback is working with. Other people will write the
'act_on_word()' function and probably many other functions like
it. The callback is actually a flex point, and not of interest right
now.

The function under test is the 'words()' function and we want to make
sure it walks the sentence correctly, dispatching individual words as
it goes. How to test this?

Let's start with a one word sentence. In this case we would expect the
callback to be invoked once with the only word, right? Here is the
test for that...

[source,c]
---------------------------------
...
#include <cgreen/mocks.h>
#include <stdlib.h>
...
void mocked_callback(const char *word, void *memo) {
  mock(word, memo);
}

Ensure(single_word_sentence_invokes_callback_once) {
  expect(mocked_callback,
    when(word, is_equal_to_string("Word")), when(memo, is_equal_to(NULL)));
  words("Word", &mocked_callback, NULL);
}

TestSuite *words_tests() {
  TestSuite *suite = create_test_suite();
  ...
  add_test(suite, single_word_sentence_invokes_callback_once);
  return suite;
}
---------------------------------

What is the funny looking 'mock()' function?

A mock is basically a programmable object. In C objects are limited to
functions, so this is a mock function. The macro 'mock()' compares the
incoming parameters with any expected values and dispatches messages
to the test suite if there is a mismatch. It also returns any values
that have been preprogrammed in the test.
 
The test function is
'single_word_sentence_invokes_callback_once()'. Using the 'expect()'
macro it programs the mock function to expect a single call. That call
will have parameters "Word" and 'NULL'. If they don't match later, we
will get a test failure.

Only the test method, not the mock callback, should be added to the
test suite.

For a successful compile and link, the +words.h+ file must now look like...

[source,c]
----------------------------
int split_words(char *sentence);
void words(const char *sentence, void (*walker)(const char *, void *), void *memo);
----------------------------

...and the +words.c+ file should have the stub...

[source,c]
----------------------------
void words(const char *sentence, void (*walker)(const char *, void *), void *memo) {
}
----------------------------

This gives us the expected failing tests...

----------------------------
Running "main" (3 tests)...
words_test.c:27: Test Failure: -> words_tests -> single_word_sentence_invokes_callback_once 
	Expected call was not made to function [mocked_callback]
Completed "main": 2 passes, 1 failure, 0 exceptions.
----------------------------

*Cgreen* reports that the callback was never invoked. We can easily get
the test to pass by filling out the implementation with...

[source,c]
----------------------------
void words(const char *sentence, void (*walker)(const char *, void *), void *memo) {
  (*walker)(sentence, memo);
}
----------------------------

That is, we just invoke it once with the whole string. This is a
temporary measure to get us moving. Now everything should pass,
although it's not much of a test yet.

That was all pretty conventional, but let's tackle the trickier case
of actually splitting the sentence. Here is the test function we will
add to +words_test.c+...

[source,c]
----------------------------
Ensure(phrase_invokes_callback_for_each_word) {
  expect(mocked_callback, when(word, is_equal_to_string("Birds")));
  expect(mocked_callback, when(word, is_equal_to_string("of")));
  expect(mocked_callback, when(word, is_equal_to_string("a")));
  expect(mocked_callback, when(word, is_equal_to_string("feather")));
  words("Birds of a feather", &mocked_callback, NULL);
}
----------------------------

Each call is expected in sequence. Any failures, or left over calls,
or extra calls, and we get failures. We can see all this when we run
the tests...

----------------------------
Running "main" (4 tests)...
words_test.c:32: Test Failure: -> words_tests -> phrase_invokes_callback_for_each_word
        Expected [[word] parameter in [mocked_callback]] to [equal string] ["Birds"]
                actual value:   ["Birds of a feather"]
                expected value: ["Birds"]
words_test.c:33: Test Failure: -> words_tests -> phrase_invokes_callback_for_each_word
        Expected call was not made to function [mocked_callback]
words_test.c:34: Test Failure: -> words_tests -> phrase_invokes_callback_for_each_word
        Expected call was not made to function [mocked_callback]
words_test.c:35: Test Failure: -> words_tests -> phrase_invokes_callback_for_each_word
        Expected call was not made to function [mocked_callback]
Completed "main": 4 passes, 4 failures, 0 exceptions.
-----------------------------

The first failure tells the story. Our little 'words()' function
called the mock callback with the entire sentence. This makes sense,
because that was the hack to get to the next test.

Although not relevant to this guide, I cannot resist getting these
tests to pass.  Besides, we get to use the function we created
earlier...

[source,c]
-----------------------------
void words(const char *sentence, void (*walker)(const char *, void *), void *memo) {
  char *words = strdup(sentence);
  int word_count = split_words(words);
  char *word = words;
  while (word_count-- > 0) {
    (*walker)(word, memo);
    word = word + strlen(word) + 1;
  }
  free(words);
}
------------------------------

And with some work we are rewarded with...

------------------------------
Running "main" (4 tests)...
Completed "main": 8 passes, 0 failures, 0 exceptions.
------------------------------

More work than I like to admit as it took me three goes to get this
right. I firstly forgot the '+ 1' added on to 'strlen()', then forgot
to swap 'sentence' for 'word' in the '(*walker)()' call, and finally
third time lucky. Of course running the tests each time made these
mistakes very obvious. It's taken me far longer to write these
paragraphs than it has to write the code.


Building Cgreen test suites
---------------------------

*Cgreen* is a tool for building unit tests in the C language. These are
usually written alongside the production code by the programmer to
prevent bugs. Even though the test suites are created by software
developers, they are intended to be human readable C code, as part of
their function is an executable specification.  Used in this way, the
test harness delivers constant quality assurance.

In other words you'll get less bugs.

Writing basic tests
~~~~~~~~~~~~~~~~~~~

*Cgreen* tests are simply C functions with no parameters and no
return value. To signal that they actually are tests we mark them with
the +Ensure+ macro. An example might be...

[source,c]
-----------------------------
Ensure(strlen_of_hello_is_five) {
    assert_that(strlen("Hello"), is_equal_to(5));
}
-----------------------------

The test name can be anything you want as long as it fullfills the
rules for an identifier in C.

The 'assert_that()' call is the primary part of an assertion, which is
complemented with a constraint, in this case 'is_equal_to()'. This
makes a very fluent interface to the asserts, that actually reads like
english.

Assertions send messages to *Cgreen*, which in turn
outputs the results.

Here are the standard constraints...

|=========================================================
|*Constraint* |*Passes if actual value/expression...*
|is_true | evaluates to true
|is_false | evaluates to false
|is_null | equals null
|is_non_null | is a non null value
|is_equal_to(value) |'== value'
|is_not_equal_to(value) |'!= value'
|is_greater_than(value) |'> value'
|is_less_than(value) |'< value'
|is_equal_to_contents_of(pointer, size)|matches the data pointed to by 'pointer' to a size of 'size' bytes
|is_not_equal_to_contents_of(pointer, size)|does not match the data pointed to by 'pointer' to a size of 'size' bytes
|is_equal_to_string(value) |are equal when compared using 'strcmp()'
|is_not_equal_to_string(value) |are not equal when compared using 'strcmp()'
|contains_string(value) |contains 'value' when evaluated using 'strstr()'
|does_contain_string(value) |does not contain 'value' when evaluated using 'strstr()'
|begins_with_string(value) |starts with the string 'value'
|is_equal_to_double(value) |are equal to 'value' within the number of significant digits (you can set 'significant_figures_for_assert_double_are(int figures)')
|is_not_equal_to_double(value) |are not equal to 'value' within the number of significant digits
|=========================================================

The boolean assertion macros accept an 'int' value. The equality
assertions accept anything that can be cast to 'intptr_t' and simply
perform an '==' operation. The string comparisons are slightly
different in that they use the '<string.h>' library function
'strcmp()'.  If 'is_equal_to()' is used on 'char *' pointers then the
pointers have to point at the same string to pass.

A cautionary note about the constraints is that you cannot use C/C++
string literal concatenation (like "don't" "use" "string"
"concatenation") in the parameters to the constraints. If you do, you
will get weird error messages about missing arguments to the
constraint macros. This is caused by the macros using argument strings
to produce nice failure messages.

Legacy style assertions
~~~~~~~~~~~~~~~~~~~~~~~

Cgreen have been around for a while, developed and matured. There is
another style of assertions that was the initial version, a style
that we now call the 'legacy style'. If you are not interested in
historical artifacts, I recommend that you skip this section.

But for completness, here are the legacy style assertion macros:

|=========================================================
|*Assertion* |*Description*
|assert_true(boolean) |Passes if boolean evaluates true
|assert_false(boolean) |Fails if boolean evaluates true
|assert_equal(first, second) |Passes if 'first == second'
|assert_not_equal(first, second) |Passes if 'first != second'
|assert_string_equal(char *, char *) |Uses 'strcmp()' and passes if the strings are equal
|assert_string_not_equal(char *, char *) |Uses 'strcmp()' and fails if the strings are equal
|=========================================================

Each assertion has a default message comparing the two values. If you
want to substitute your own failure messages, then you must use the
'*_with_message()' counterparts...

|=========================================================
|*Assertion*
|assert_true_with_message(boolean, message, ...)
|assert_false_with_message(boolean, message, ...)
|assert_equal_with_message(tried, expected, message, ...)
|assert_not_equal_with_message(tried, unexpected, message, ...)
|assert_string_equal_with_message(char *, char *, message, ...)
|assert_string_not_equal_with_message(char *, char *, message, ...)
|=========================================================

All these assertions have an additional 'char *' message parameter,
which is the message you wished to display on failure. If this is set
to 'NULL', then the default message is shown instead. The most useful
assertion from this group is 'assert_true_with_message()' as you can
use that to create your own assertion functions with your own
messages.

Actually the assertion macros have variable argument lists. The
failure message acts like the template in 'printf()'. We could change
the test above to be...

[source,c]
-----------------------------
Ensure(strlen_of_hello_is_five) {
    const char *greeting = "Hello";
    int length = strlen(greeting);
    assert_equal_with_message(length, 5, "[%s] should be 5, but was %d", greeting, length);
}
-----------------------------

A slightly more user friendly message when things go wrong. But,
actually, Cgreens default messages are so good that you are
encouraged to skip the legacy style and go for the more modern
constaints style assertions.

A runner
~~~~~~~~

For the tests to actually be run there needs to be a running test
suite. (But see also <<runner, Automatic Test Discovery>>.) We can
create one especially for this test like so...

[source,c]
-----------------------------
TestSuite *our_tests() {
    TestSuite *suite = create_test_suite();
    add_test(suite, strlen_of_hello_is_five);
    return suite;
}
-----------------------------

In case you have spotted that 'strlen_of_hello_is_five()' should have
an ampersand in front of it, 'add_test()' is actually a macro. The '&'
is added automatically. Further more, the Ensure-macro actually
mangles the tests name, so it is not actually a function name. (This
might also make them a bit difficult to find in the debugger....)

To run the test suite, we call 'run_test_suite()' on it. So we can
just write...

[source,c]
-----------------------------
run_test_suite(our_tests(), create_text_reporter());
-----------------------------

The results of assertions are ultimately delivered as passes and
failures to a collection of callbacks defined in a 'TestReporter'
structure. There is a predefined 'TestReporter' in *Cgreen* called the
'TextReporter' that delivers messages in plain text like we have
already seen.

The complete test code now looks like...

[source,c]
-----------------------------
#include <cgreen/cgreen.h>
#include <string.h>

Ensure(strlen_of_hello_is_five) {
    assert_that(strlen("Hello"), is_equal_to(5));
}

TestSuite *our_tests() {
    TestSuite *suite = create_test_suite();
    add_test(suite, strlen_of_hello_is_five);
    return suite;
}

int main(int argc, char **argv) {
    return run_test_suite(our_tests(), create_text_reporter());
}
-----------------------------

The return value of 'run_test_suite()' is a Unix exit code.

Compiling and running gives...

-----------------------------
gcc -c strlen_test.c
gcc strlen_test.o -lcgreen -o strlen_test
./strlen_test
Running "our_tests" (1 tests)...
Completed "our_tests": 1 pass, 0 failures, 0 exceptions.
-----------------------------

The test messages are only shown on failure. If we break our test to see it...

[source,c]
-----------------------------
Ensure(strlen_of_hello_is_five) {
    assert_that(strlen("Hiya", is_equal_to(5));
}
-----------------------------

...we'll get the helpful message...

-----------------------------
Running "our_tests" (1 tests)...
strlen_test.c:5: Failure: -> strlen_of_hello_is_five 
	Expected [strlen("Hiya")] to [equal] [5]
		actual value:	[4]
		expected value:	[5]
Completed "our_tests": 0 passes, 1 failure, 0 exceptions.
-----------------------------
                
*Cgreen* starts every message with the location of the test failure so
that the usual error message identifying tools (like emacs
+next-error+) will work out of the box.

Once we have a basic test scaffold up, it's pretty easy to add more
tests. Adding a test of 'strlen()' with an empty string for example...

[source,c]
-----------------------------
...
Ensure(strlen_of_empty_string_is_zero) {
    assert_equal(strlen("\0"), 0);
}

TestSuite *our_tests() {
    TestSuite *suite = create_test_suite();
    add_test(suite, strlen_of_hello_is_five);
    add_test(suite, strlen_of_empty_string_is_zero);
    return suite;
}
...
-----------------------------

And so on.

Set up and tear down
~~~~~~~~~~~~~~~~~~~~

It's common for test suites to have a lot of duplicate code,
especially when setting up similar tests. Take this database code for
example...

[source,c]
-----------------------------
#include <cgreen/cgreen.h>
#include <stdlib.h>
#include <mysql/mysql.h>
#include "person.h"

static void create_schema() {
    MYSQL *connection = mysql_init(NULL);
    mysql_real_connect(connection, "localhost", "me", "secret", "test", 0, NULL, 0);
    mysql_query(connection, "create table people (name, varchar(255) unique)");
    mysql_close(connection);
}

static void drop_schema() {
    MYSQL *connection = mysql_init(NULL);
    mysql_real_connect(connection, "localhost", "me", "secret", "test", 0, NULL, 0);
    mysql_query(connection, "drop table people");
    mysql_close(connection);
}

Ensure(can_add_person_to_database) {
    create_schema();
    Person *person = create_person();
    set_person_name(person, "Fred");
    save_person(person);
    Person *found = find_person_by_name("Fred");
    assert_that(get_person_name(person), is_equal_to_string("Fred"));
    drop_schema();
}

Ensure(cannot_add_duplicate_person) {
    create_schema();
    Person *person = create_person();
    set_person_name(person, "Fred");
    assert_that(save_person(person), is_true);
    Person *duplicate = create_person();
    set_person_name(duplicate, "Fred");
    assert_that(save_person(duplicate), is_false);
    drop_schema();
}

TestSuite *person_tests() {
    TestSuite *suite = create_test_suite();
    add_test(suite, can_add_person_to_database);
    add_test(suite, cannot_add_duplicate_person);
    return suite;
}

int main(int argc, char **argv) {
    return run_test_suite(person_tests(), create_text_reporter());
}
--------------------------

We have already factored out the duplicate code into it's own
functions 'create_scheme()' and 'drop_schema()', so things are not so
bad. At least not yet. What happens when we get dozens of tests? For a
test subject as complicated as a database
http://www.martinfowler.com/eaaCatalog/activeRecord.html[ActiveRecord],
having dozens of tests is very likely.

We can get *Cgreen* to do some of the work for us by declaring these
methods as 'setup' and 'teardown' functions in the test suite.
 
Here is the new version...

[source,c]
-----------------------------
...
static void create_schema() { ... }

static void drop_schema() { ... }

Ensure(can_add_person_to_database) {
    Person *person = create_person();
    set_person_name(person, "Fred");
    save_person(person);
    Person *found = find_person_by_name("Fred");
    assert_that(get_person_name(person), is_equal_to_string("Fred"));
}

Ensure(cannot_add_duplicate_person) {
    Person *person = create_person();
    set_person_name(person, "Fred");
    assert_that(save_person(person), is_true);
    Person *duplicate = create_person();
    set_person_name(duplicate, "Fred");
    assert_that(save_person(duplicate), is_false);
}

TestSuite *person_tests() {
    TestSuite *suite = create_test_suite();
    set_setup(suite, create_schema);
    set_teardown(suite, drop_schema);
    add_test(suite, can_add_person_to_database);
    add_test(suite, cannot_add_duplicate_person);
    return suite;
}
...
---------------------------

With this new arrangement *Cgreen* runs the 'create_schema()' function
before each test, and the 'drop_schema()' function after each
test. This saves some repetitive typing and reduces the chance of
accidents. It also makes the tests more focused.

The reason we try so hard to strip everything out of the test
functions is the fact that the test suite acts as documentation. In
our +person.h+ example we can easily see that 'Person' has some kind
of name property, and that this value must be unique. For the tests to
act like a readable specification we have to remove as much mechanical
clutter as we can.

A couple of details. You can have only one 'setup' and one 'teardown'
in each 'TestSuite' as indicated by the names 'set_setup()' and
'set_teardown()'. Also the 'teardown' function may not be run if
the test crashes, causing some test interference. This brings us
nicely onto the next section...

Each test in it's own process
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider this test method...

[source,c]
-----------------------------
Ensure(will_seg_fault) {
    int *p = NULL;
    (*p)++;
}
-----------------------------

Crashes are not something you would normally want to have in a test
run. Not least because it will stop you receiving the very test output
you need to tackle the problem.

To prevent segmentation faults and other problems bringing down the
test suites, *Cgreen* runs every test in it's own process.

Just before calling the 'setup' function, *Cgreen* 'fork()''s. The main
process wait's for the test to complete normally or die. This includes
the calling the 'teardown' function, if any. If the test process dies,
an exception is reported and the main test process carries on.

For example...

[source,c]
-----------------------------
#include <cgreen/cgreen.h>
#include <stdlib.h>

Ensure(will_seg_fault) {
    int *p = NULL;
    (*p)++;
}

int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_test(suite, will_seg_fault);
    run_test_suite(suite, create_text_reporter());
}
-----------------------------


When built and run, this gives...

-----------------------------
Running "main" (1 tests)...
crash_test.c:4: Exception: -> will_seg_fault
        Test exited unexpectedly, likely from a non-standard exception, SIGSEGV, or other signal
Completed "main": 0 passes, 0 failures, 1 exception.
-----------------------------

The obvious thing to do now is to fire up the debugger. Unfortunately,
the constant 'fork()''ing of *Cgreen* can be an extra complication too
many when debugging. It's enough of a problem to find the bug.

To get around this, and also to allow the running of one test at a
time, *Cgreen* has the 'run_single_test()' function. The signatures of
the two run methods are...

- 'int run_test_suite(TestSuite *suite, TestReporter *reporter);'
- 'int run_single_test(TestSuite *suite, char *test, TestReporter *reporter);'

The extra parameter of 'run_single_test()', the 'test' string, is the
name of the test to select.  This could be any test, even in nested
test suites (see below). Here is how we would use it to debug our
crashing test...

[source,c]
-----------------------------
int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_test(suite, will_seg_fault);
    run_single_test(suite, "will_seg_fault", create_text_reporter());
}
-----------------------------

When run in this way, *Cgreen* will not 'fork()'.

This deals with the segmentation fault case, but what about a process
that fails to complete by getting stuck in a loop?

Well, *Cgreen* will wait forever too. Using the C signal handlers, we
can place a time limit on the process by sending it an interrupt. To
save us writing this ourselves, *Cgreen* includes the 'die_in()'
function to help us out.

Here is an example of time limiting a test...

[source,c]
-----------------------------
...
Ensure(will_seg_fault) { ... }

Ensure(this_would_stall) {
    die_in(1);
    while(0 == 0) { }
}

int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_test(suite, will_seg_fault);
    add_test(suite, this_would_stall);
    run_test_suite(suite, create_text_reporter());
}
---------------------------
                
When executed, the code will slow for a second, and then finish with...

---------------------------
Running "main" (2 tests)...
crash_test.c:4: Exception: -> will_seg_fault
        Test exited in unexpectedly, likely from a non-standard exception, SIGSEGV, or other signal
crash_test.c:9: Exception: -> will_stall
        Test exited in unexpectedly, likely from a non-standard exception, SIGSEGV, or other signal
Completed "main": 0 passes, 0 failures, 2 exceptions.
---------------------------

Note that you see the test results as they come in. *Cgreen* streams the
results as they happen, making it easier to figure out where the test
suite has problems.

Of course, if you want to set a general time limit on all your tests,
then you can add a 'die_in()' to a 'setup()' function. *Cgreen* will
then apply the limit to all of them.

Building composite test suites
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            
The 'TestSuite' is a composite structure.  This means test suites can
be added to test suites, building a tree structure that will be
executed in order.

Let's combine the 'strlen()' tests with the 'Person' tests above.
Firstly we need to remove the 'main()' calls.  E.g...

[source,c]
-----------------------------
#include <cgreen/cgreen.h>
#include <string.h>

Ensure(strlen_of_hello_is_five) { ... }
Ensure(strlen_of_empty_string_is_zero) { ... }

TestSuite *our_tests() {
    TestSuite *suite = create_test_suite();
    add_test(suite, strlen_of_hello_is_five);
    add_test(suite, strlen_of_empty_string_is_zero);
    return suite;
}
----------------------------

Then we can write a small runner script with a new
'main()' function...

[source,c]
-----------------------------
#include "strlen_tests.c"
#include "person_tests.c"

TestSuite *our_tests();
TestSuite *person_tests();

int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_suite(suite, our_tests());
    add_suite(suite, person_tests());
    if (argc > 1) {
        return run_single_test(suite, argv[1], create_text_reporter());
    }
    return run_test_suite(suite, create_text_reporter());
}
-----------------------

It's usually easier to place the 'TestSuite' prototypes in the runner
scripts, rather than have lot's of header files.  This is the same
reasoning that let us drop the prototypes for the test functions in
the actual test scripts.  We can get away with this, because the tests
are more about documentation than encapsulation.

It's sometimes handy to be able to run just a single test from the
command line, so we added a simple 'if' block to take the test name as
an optional argument.  The entire test suite will be searched for the
named test.  This trick also saves us a recompile when we debug.

We've placed each test suite in it's own file, but that is not
necessary.  We could build several test suites in the same file, even
nesting them.  We can even add mixtures of test functions and test
suites to the same parent test suite.  Loops will give trouble,
however.

If we do place several suites in the same file, then all the suites
will be named the same in the breadcrumb trail in the test message.
They will all be named after the function the create call sits in.  If
you want to get around this, or you just like to name your test
suites, you can use 'create_named_test_suite()' instead of
'create_test_suite()'.  This takes a single string parameter.  In fact
'create_test_suite()' is just a macro that inserts the '__func__'
constant into 'create_named_test_suite()'.

What happens to 'setup' and 'teardown' functions in a 'TestSuite' that
contains other 'TestSuite's?

Well firstly, *Cgreen* does not 'fork()' when running a suite.  It
leaves it up to the child suite to 'fork()' the individual tests.
This means that a 'setup' and 'teardown' will run in the main
process.  They will be run once for each child suite.

We can use this to speed up our 'Person' tests above.  Remember we
were creating a new connection and closing it again in the fixtures.
This means opening and closing a lot of connections.  At the slight
risk of some test interference, we could reuse the connection accross
tests...

[source,c]
-----------------------
...
static MYSQL *connection;

static void create_schema() {
    mysql_query(connection, "create table people (name, varchar(255) unique)");
}

static void drop_schema() {
    mysql_query(connection, "drop table people");
}

Ensure(can_add_person_to_database) { ... }
Ensure(cannot_add_duplicate_person) { ... }

void open_connection() {
    connection = mysql_init(NULL);
    mysql_real_connect(connection, "localhost", "me", "secret", "test", 0, NULL, 0);
}

void close_connection() {
    mysql_close(connection);
}

TestSuite *person_tests() {
    TestSuite *suite = create_test_suite();
    set_setup(suite, create_schema);
    set_teardown(suite, drop_schema);
    add_test(suite, can_add_person_to_database);
    add_test(suite, cannot_add_duplicate_person);

    TestSuite *fixture = create_named_test_suite("Mysql fixture");
    add_suite(fixture, suite);
    set_setup(fixture, open_connection);
    set_teardown(fixture, close_connection);
    return fixture;
}
-----------------------

The trick here is creating a test suite as a wrapper whose sole
purpose to wrap the main test suite in the fixture.  This is our
'fixture' pointer.  This code is a little confusing, because we have
two sets of fixtures in the same test script.

We have the MySQL connection fixture.  This is runs
'open_connection()' and 'close_connection()' just once at the
beginning and end of the person tests.  This is because the 'suite'
pointer is the only member of 'fixture'.

We also have the schema fixture, the 'create_schema()' and
'drop_schema()', which is run before and after every test.  Those are
still attached to the inner 'suite'.

In the real world we would probably place the connection
fixture in it's own file...

[source,c]
-----------------------
static MYSQL *connection;

MYSQL *get_connection() {
    return connection;
}

static void open_connection() {
    connection = mysql_init(NULL);
    mysql_real_connect(connection, "localhost", "me", "secret", "test", 0, NULL, 0);
}

static void close_connection() {
    mysql_close(connection);
}

TestSuite *connection_fixture(TestSuite *suite) {
    TestSuite *fixture = create_named_test_suite("Mysql fixture");
    add_suite(fixture, suite);
    set_setup(fixture, open_connection);
    set_teardown(fixture, close_connection);
    return fixture;
}
-----------------------

This allows the reuse of common fixtures across projects.
             
Mocking functions with Cgreen
-----------------------------
        
When testing you want certainty above all else.  Random events destroy
confidence in your test suite and force needless extra runs "to be
sure".  A good test places the subject under test into a tightly
controlled environment.  A test chamber if you like.  This makes the
tests fast, repeatable and reliable.

To create a test chamber for testing code, we have to control any
outgoing calls from the code under test.  We won't believe our test
failure if our code is making calls to the internet for example.  The
internet can fail all by itself.  Not only do we not have total
control, but it means we have to get dependent components working
before we can test the higher level code.  This makes it difficult to
code top down.

The solution to this dilemma is to write stub code for the components
whilst the higher level code is written.  This pollutes the code base
with temporary code, and the test isolation disappears when the system
is eventually fleshed out.

The ideal is to have minimal stubs written for each individual test.
*Cgreen* encourages this approach by making such tests easier to write.
         
The problem with streams
~~~~~~~~~~~~~~~~~~~~~~~~
            
How do we test this code...?

[source,c]
-----------------------
char *read_paragraph(int (*read)(void *), void *stream) {
    int buffer_size = 0, length = 0;
    char *buffer = NULL;
    int ch;
    while ((ch = (*read)(stream)) != EOF) {
        if (++length > buffer_size) {
            buffer_size += 100;
            buffer = (char *)realloc(buffer, buffer_size + 1);
        }
        if ((buffer[length] = ch) == '\n') {
            break;
        }
        buffer[length + 1] = '\0';
    }
    return buffer;
}
-----------------------

This is a fairly generic stream filter that turns the incoming
characters into C string paragraphs.  Each call creates one paragraph,
returning a pointer to it or returning 'NULL' if there is no
paragraph.  The paragraph has memory allocated to it and the stream is
advanced ready for the next call.  That's quite a bit of
functionality, and there are plenty of nasty boundary conditions.  I
really want this code tested before I deploy it.

The problem is the stream dependency.  We could use a real stream, but
that will cause all sorts of headaches.  It makes the test of our
paragraph formatter dependent on a working stream.  It means we have
to write the stream first, bottom up coding rather than top down.  It
means we will have to simulate stream failures - not easy.  It will
also mean setting up external resources.  This is more work, will run
slower, and could lead to spurious test failures.

By contrast we could write a simulation of the stream for each test,
called a "server stub".

For example, when the stream is empty nothing should happen.  We
hopefully get 'NULL' from 'read_paragraph' when the stream is
exhausted.  That is, it just returns a steady stream of 'EOF's.

[source,c]
-----------------------
static int empty_stream(void *stream) {
    return EOF;
}

Ensure(reading_lines_from_empty_stream_gives_null) {
    assert_that(read_paragraph(&empty_stream, NULL), is_null);
}

TestSuite *stream_tests() {
    TestSuite *suite = create_test_suite();
    add_test(suite, reading_lines_from_empty_stream_gives_null);
    return suite;
}
-----------------------

Our simulation is easy here, because our fake stream returns only one
value.  Things are harder when the function result changes from call
to call as a real stream would.  Simulating this would mean messing
around with static variables and counters that are reset for each
test.  And of course, we will be writing quite a few stubs.  Often a
different one for each test.  That's a lot of clutter.

*Cgreen* handles this clutter for us by letting us write a single
programmable function for all our tests.
             
Record and playback
~~~~~~~~~~~~~~~~~~~
            
We can redo our example by creating a 'stub_stream()' function
(any name will do)...

[source,c]
-----------------------
static int stub_stream(void *stream) {
    return (int)mock();
}
-----------------------

Hardly longer that our trivial server stub above, it is just a macro
to generate a return value, but we can reuse this in test after test.

For our simple example above we just tell it to always return 'EOF'...

[source,c]
-----------------------
#include <cgreen/mocks.h>

static int stub_stream(void *stream) {
    return (int)mock(stream);
}

Ensure(reading_lines_from_empty_stream_gives_null) {
    always_expect(stub_stream, will_return(EOF));
    assert_that(read_paragraph(stub_stream, NULL), is_null);
}
-----------------------

The 'always_expect()' macro takes as arguments the function name and
the 'will_return()' defines the return value. This is an expectation
of a call to the stub, and we have told 'stub_stream()' to always
return 'EOF' when called.


Let's see if our production code actually works...

-----------------------
Running "stream_tests" (1 tests)...
Completed "stream_tests": 1 pass, 0 failures, 0 exceptions.
-----------------------

So far, so good.  On to the next test.

If we want to test a one character line, we have to send the
terminating 'EOF' or '"\n"' as well as the single character.
Otherwise our code will loop forever, giving an infinite line of that
character.


Here is how we can do this...

[source,c]
-----------------------
Ensure(one_character_stream_gives_one_character_line) {
    expect(stub_stream, will_return('a'));
    expect(stub_stream, will_return(EOF));
    char *line = read_paragraph(&stub_stream, NULL);
    assert_that(line, is_equal_to_string("a"));
    free(line);
}
-----------------------

Unlike the 'always_expect()' instruction, 'expect()' sets up an
expectation of a single call and specifying 'will_return()' sets the
single return value for just that call.  It acts like a record and
playback model.  Successive expectations map out the return sequence
that will be given back once the test proper starts.

We'll add this test to the suite and run it...

-----------------------
Running "stream_tests" (2 tests)...
stream_test.c:19: Failure: -> one_character_stream_gives_one_character_line 
	Expected [line] to [equal string] ["a"]
		actual value:	["8a"]
		expected value:	["a"]
Completed "stream_tests": 1 pass, 1 failure, 0 exceptions.
-----------------------

Oops. Our code under test doesn't work. Already we need a fix...

[source,c]
-----------------------
char *read_paragraph(int (*read)(void *), void *stream) {
    int buffer_size = 0, length = 0;
    char *buffer = NULL;
    int ch;
    while ((ch = (*read)(stream)) != EOF) {
        if (++length > buffer_size) {
            buffer_size += 100;
            buffer = (char *)realloc(buffer, buffer_size + 1);
        }
        if ((buffer[length - 1] = ch) == '\n') {
            break;
        }
        buffer[length] = '\0';
    }
    return buffer;
}
-----------------------

After which everything is fine...

-----------------------
Running "stream_tests" (2 tests)...
Completed "stream_tests": 2 passes, 0 failures, 0 exceptions.
-----------------------
             
How do the *Cgreen* stubs work?  Each 'expect()' describes one call to
the stub and the 'will_return()' calls build up a static list of
return values which are used and returned in order as those calls
arrive. The return values are cleared between tests.

The 'mock()' macro captures the parameter names and the '__func__'
property (the name of the stub function).  *Cgreen* can then use these
to look up entries in the return list, and also to generate more
helpful messages.

We can crank out our tests quite quickly now...

[source,c]
-----------------------
Ensure(one_word_stream_gives_one_word_line) {
    expect(stub_stream, will_return('t'));
    expect(stub_stream, will_return('h'));
    expect(stub_stream, will_return('e'));
    always_expect(stub_stream, will_return(EOF));
    assert_that(read_paragraph(&stub_stream, NULL), is_equal_to_string("the"));
}
-----------------------

I've been a bit naughty.  As each test runs in it's own process, I
haven't bothered to free the pointers to the paragraphs.  I've just
let the operating system do it.  Purists may want to add the extra
clean up code.

I've also used 'always_expect()' for the last instruction.  Without
this, if the stub is given an instruction it does not expect, it will
throw a test failure.  This is overly restrictive, as our
'read_paragraph()' function could quite legitimately call the stream
after it had run off of the end.  OK, that would be odd behaviour, but
that's not what we are testing here.  If we were, it would be placed
in a test of it's own.  The 'always_expect()' call tells *Cgreen* to
keep going after the first three letters, allowing extra calls.

As we build more and more tests, they start to look like a
specification of the wanted behaviour...

[source,c]
-----------------------
Ensure(drops_line_ending_from_word_and_stops) {
    expect(stub_stream, will_return('t'));
    expect(stub_stream, will_return('h'));
    expect(stub_stream, will_return('e'));
    expect(stub_stream, will_return('\n'));
    assert_that(read_paragraph(&stub_stream, NULL), is_equal_to_string("the"));
}
-----------------------

...and just for luck...

[source,c]
-----------------------
Ensure(single_line_ending_gives_empty_line) {
    expect(stub_stream, will_return('\n'));
    assert_that(read_paragraph(&stub_stream, NULL), is_equal_to_string(""));
}
-----------------------

This time we musn't use 'always_return()'.  We want to leave the
stream where it is, ready for the next call to 'read_paragraph()'.  If
we call the stream beyond the line ending, we want to fail.
             
Oops, that was a little too fast. Turns out we are failing anyway...

-----------------------
Running "stream_tests" (5 tests)...
stream_test.c:36: Failure: -> drops_line_ending_from_word_and_stops 
	Expected [read_paragraph(&stub_stream, NULL)] to [equal string] ["the"]
		actual value:	["the
"]
		expected value:	["the"]
stream_test.c:41: Failure: -> single_line_ending_gives_empty_line 
	Expected [read_paragraph(&stub_stream, NULL)] to [equal string] [""]
		actual value:	["
öjj"]
		expected value:	[""]
Completed "stream_tests": 3 passes, 2 failures, 0 exceptions.
-----------------------

Clearly we are passing through the line ending.
Another fix later...

[source,c]
-----------------------
char *read_paragraph(int (*read)(void *), void *stream) {
    int buffer_size = 0, length = 0;
    char *buffer = NULL;
    int ch;
    while ((ch = (*read)(stream)) != EOF) {
        if (++length > buffer_size) {
            buffer_size += 100;
            buffer = (char *)realloc(buffer, buffer_size + 1);
        }
        if ((buffer[length - 1] = ch) == '\n') {
            buffer[--length] = '\0';
            break;
        }
        buffer[length] = '\0';
    }
    return buffer;
}
-----------------------

And we are passing again...

-----------------------
Running "stream_tests" (5 tests)...
Completed "stream_tests": 5 passes, 0 failures, 0 exceptions.
-----------------------
             
There are no limits to the number of stubbed methods within a test,
only that two stubs cannot have the same name.  So the following will
cause problems...

[source,c]
-----------------------
static int stub_stream(void *stream) {
    return (int)mock();
}

Ensure(bad_test) {
    expect(stub_stream, will_return('a'));
    do_stuff(&stub_stream, &stub_stream);
}
-----------------------

It will be necessary to have two stubs to make this test behave...

[source,c]
-----------------------
static int first_stream(void *stream) {
    return (int)mock();
}

static int second_stream(void *stream) {
    return (int)mock();
}

Ensure(good_test) {
    expect(first_stream, will_return('a'));
    expect(second_stream, will_return('a');
    do_stuff(&first_stream, &second_stream);
}
-----------------------

We now have a way of writing fast, clear tests with no external
dependencies.  The information flow is still one way though, from stub
to the code under test.  When our code calls complex procedures, we
won't want to pick apart the effects to infer what happened.  That's
too much like detective work.  And why should we?  We just want to
know that we dispatched the correct information down the line.

Things get more interesting when we thing of the traffic going the
other way, from code to stub.  This gets us into the same territory as
mock objects.
             
Setting expectations on mock functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            
To swap the traffic flow, we'll look at an outgoing example instead.
Here is the prewritten production code...

[source,c]
-----------------------
void by_paragraph(int (*read)(void *), void *in, void (*write)(void *, char *), void *out) {
    while (1) {
        char *line = read_paragraph(read, in);
        if (line == NULL) {
            return;
        }
        (*write)(out, line);
        free(line);
    }
}
-----------------------

This is the start of a formatter utility.  Later filters will probably
break the paragaphs up into justified text, but right now that is all
abstracted behind the 'void write(void *, char *)' interface.  Our
current interests are: does it loop through the paragraphs, and does
it crash?

We could test correct paragraph formation by writing a stub that
collects the paragraphs into a 'struct'.  We could then pick apart
that 'struct' and test each piece with assertions.  This approach is
extremely clumsy in C.  The language is just not suited to building
and tearing down complex edifices, never mind navigating them with
assertions.  We would badly clutter our tests.

Instead we'll test the output as soon as possible, right in
the called function...

[source,c]
-----------------------
...
void expect_one_letter_paragraph(char *paragraph, void *stream) {
    assert_string_equal(paragraph, "a", NULL);
}

Ensure(one_character_is_made_into_a_one_letter_paragraph) {
    by_paragraph(
            &one_character_stream,
            NULL,
            &expect_one_letter_paragraph,
            NULL);
}
...
-----------------------

By placing the assertions into the mocked function, we keep the tests
minimal.  The catch with this method is that we are back to writing
individual functions for each test.  We have the same problem as we
had with hand coded stubs.

Again, *Cgreen* has a way to automate this.  Here is the rewritten
test...

[source,c]
-----------------------
static int reader(void *stream) {
    return (int)mock(stream);
}

static void writer(void *stream, char *paragraph) {
    mock(stream, paragraph);
}

Ensure(one_character_is_made_into_a_one_letter_paragraph) {
    expect(reader, will_return('a'));
    always_expect(reader, will_return(EOF));
    expect(writer, when(paragraph, is_equal_to_string("a")));
    by_paragraph(&reader, NULL, &writer, NULL);
}
-----------------------

Where are the assertions?

Unlike our earlier stub, 'reader()' can now check it's parameters.  In
object oriented circles, an object that checks it's parameters as well
as simulating behaviour is called a mock object.  By analogy
'reader()' is a mock function, or mock callback.

Using the 'expect' macro, we have set up the expectation that
'writer()' will be called just once.  That call must have the string
'"a"' for the 'paragraph' parameter.  If the actual value of that
parameter does not match, the mock function will issue a failure
straight to the test suite.  This is what saves us writing a lot of
assertions.

When specifying behavior of mocks there are three parts. First, how
often the specified behaviour or expectation will be executed:

|=========================================
|*Macro*|*Description*
|'expect(function, ...)'|Expected once, in order
|'always_expect(function, ...)'|Expect this behavior from here onwards
|'never_expect(function)'|This mock is expected to never be called
|=========================================

You can specify constraints and behaviours for each expectation
(except for 'never_expect()' naturally). A constraint places
restrictions on the parameters (and will tell you if the expected
restriction was not met), and a behaviour specifies what the mock
should do if the parameter constraints are met.

A parameter constraint is defined using the 'when(parameter,
constraint)' macro. It takes two parameters:

|=================================================
|*Parameter*|*Description*
|parameter|The name of the parameter to the mock function
|constraint|A constraint placed on that parameter
|=================================================

There is a multitude of constraints available (actually, exactly the
same as for the assertions we saw earlier):

|========================================
|*Constraint*|*Type*
|is_equal_to(value)| Integers
|is_not_equal_to(value)| Integers
|is_greater_than(value)| Integers
|is_less_than(value)| Integers
||
|is_equal_to_contents_of(pointer, size_of_contents)|Bytes/Structures
|is_not_equal_to_contents_of(pointer, size_of_contents)|Bytes/Structures
||
|is_equal_to_string(value)|String
|is_not_equal_to_string(value)|String
|contains_string(value)|String
|does_not_contain_string(value)|String
|begins_with_string(value)|String
||
|is_equal_to_double(value)|Double
|is_not_equal_to_double(value)|Double
|========================================

Then there are two ways to return results:

|========================================
|*Macro*|*Description*
|will_return(value)|Return the value from the mock function (which needs to be declared returning that type
|will_set_contents_of_parameter(parameter_name, value, size)|Writes the value in the referenced parameter
|========================================

You can combine these in various ways:

[source,c]
-----------------------
  expect(mocked_file_writer,
        when(data, is_equal_to(42)),
        will_return(EOF));
  expect(mocked_file_reader,
        when(file, is_equal_to_contents_of(&FD, sizeof(FD))),
        when(input, is_equal_to_string("Hello world!"),
        will_set_contents_of_parameter(status, FD_CLOSED, sizeof(bool))));
-----------------------

If multiple 'when()' are specified they all need to be fullfilled. You
can of course only have one for each of the parameters of your mock
function.

You can also have multiple 'will_set_contents_of_parameter()' in an
expectation, one for each reference parameter, but naturally only one
'will_return()'.

It's about time we actually ran our test...

-----------------------
Running "stream_tests" (6 tests)...
Completed "stream_tests": 6 passes, 0 failures, 0 exceptions.
-----------------------

Confident that a single character works, we can further specify the
behaviour.  Firstly an input sequence...

[source,c]
-----------------------
Ensure(no_line_endings_makes_one_paragraph) {
    expect(reader, will_return('a'));
    expect(reader, will_return(' '));
    expect(reader, will_return('b'));
    expect(reader, will_return(' '));
    expect(reader, will_return('c'));
    always_expect(reader, will_return(EOF));
    expect(writer, when(paragraph, is_equal_to_string("a b c")));
    by_paragraph(&reader, NULL, &writer, NULL);
}
-----------------------

A more intelligent programmer than me would place all these calls in a
loop.  Next, checking an output sequence...

[source,c]
-----------------------
Ensure(line_endings_generate_separate_paragraphs) {
    expect(reader, will_return('a'));
    expect(reader, will_return('\n'));
    expect(reader, will_return('b'));
    expect(reader, will_return('\n'));
    expect(reader, will_return('c'));
    always_expect(reader, will_return(EOF));
    expect(writer, when(paragraph, is_equal_to_string("a")));
    expect(writer, when(paragraph, is_equal_to_string("b")));
    expect(writer, when(paragraph, is_equal_to_string("c")));
    by_paragraph(&reader, NULL, &writer, NULL);
}
-----------------------

Again we can se that the 'expect()' calls follow a record and playback
model.  Each one tests a successive call.  This sequence confirms that
we get '"a"', '"b"' and '"c"' in order.

Then we'll make sure the correct stream pointers are passed to the
correct functions.  This is a more realistic parameter check...

[source,c]
-----------------------
Ensure(resources_are_paired_with_the_functions) {
    expect(reader, when(stream, is_equal_to(1)), will_return('a'));
    always_expect(reader, when(stream, is_equal_to(1)), will_return(EOF));
    expect(writer, when(stream, is_equal_to(2)));
    by_paragraph(&reader, (void *)1, &writer, (void *)2);
}
-----------------------

And finally we'll specify that the writer is not called if
there is no paragraph.

[source,c]
-----------------------
Ensure(empty_paragraphs_are_ignored) {
    expect(reader, will_return('\n'));
    always_expect(reader, will_return(EOF));
    never_expect(writer);
    by_paragraph(&reader, NULL, &writer, NULL);
}
-----------------------

This last test is our undoing...

-----------------------
Running "stream_tests" (10 tests)...
streams_tests.c:93: Test Failure: -> empty_paragraphs_are_ignored
        Function [writer] has an expectation that it will never be called, but it was
Completed "stream_tests": 14 passes, 1 failure, 0 exceptions.
-----------------------

Obviously blank lines are still being dispatched to the 'writer()'.
Once this is pointed out, the fix is obvious...

[source,c]
-----------------------
void by_paragraph(int (*read)(void *), void *in, void (*write)(void *, char *), void *out) {
    while (1) {
        char *line = read_paragraph(read, in);
        if ((line == NULL) || (strlen(line) == 0)) {
            return;
        }
        (*write)(out, line);
        free(line);
    }
}
-----------------------

Tests with 'never_expect()' can be very effective at uncovering subtle
bugs.

-----------------------
Running "stream_tests"...
Completed "stream_tests": 14 passes, 0 failures, 0 exceptions.
-----------------------

All done.

=== Mocks Are...

Using mocks is a very handy way to isolate a unit and catch and
control calls to external units. Depending on your style of coding two
schools of thinking have emerged. And of course *Cgreen* supports
both!


==== Strict or Loose Mocks ====

The two schools are thinking a bit differently about what mock
expectations means. Does it mean that all external calls must be
declared and expected? What happens if a call was made to a mock that
wasn't expected? And vice versa, if an expected call was not made?

Actually, the thinking is not only a school of thought, but you might
want to switch from one to the other. So *Cgreen* allows for that too.

By default *Cgreen* mocks are ''strict'', which means that a call to
an non-expected mock will be considered a failure. So will an expected
call that was not fullfilled. You might consider this a way to define
a unit through all its exact behaviours towards its neighbours.

On the other hand, ''loose'' mocks are looser. They allow both
unfullfilled expectations and try to handle unexpected calls in a
reasonable way.

You can use both with in the same suite of tests using the call
'cgreen_mocks_are(strict_mocks);' and 'cgreen_mocks_are(loose_mocks);'
respectively.


==== Learning Mocks ====

Working with legacy code and trying to apply TDD, BDD or even simply
add some unit tests is not easy. You're working with unknown code that
does unknown things with unknown counterparts.

So the first step would be to isolate the unit. We won't go into
details on how to do that here, but basically you would replace the
interface to other units with mocks. This is a somewhat tedious manual
labor, but will result in an isolated unit where you can start
applying your unit tests.

Once you have your unit isolated in a harness of mocks, we need to
figure out which calls it does to other units, now replaced by mocks,
in the specific case we are trying to test.

This might be complicated, so *Cgreen* makes that a bit simpler. There
is a third ''mode'' of the *Cgreen* mocks, the learning mocks.

If you temporarily add the call 'cgreen_mocks_are(learning_mocks);' at
the beginning of your unit test, the mocks will record all calls and
present a list of those calls in order, including the actual parameter
values, on the standard output.

So let's look at the following example from the *Cgreen* unit
tests. It's a bit contorted since the test actually call the mocked
functions directly, but I believe it will serve as an example.

[source,c]
-----
static char *string_out(int p1) {
    return (char *)mock(p1);
}

static int integer_out() {
    return (int)mock();
}

Ensure(Mocks, learning_mocks_emit_pastable_code) {
    cgreen_mocks_are(learning_mocks);
    string_out(1);
    string_out(2);
    integer_out();
    integer_out();
    string_out(3);
    integer_out();
}
-----

We can see the call to 'cgreen_mocks_are()' starting the test and
setting the mocks into learning mode.

If we run this, just as we usually run tests, the following will show
up in our terminal:

[source,c]
----
learning_mocks_emit_pastable_code: learned mocks:
        expect(string_out, when(p1, is_equal_to(1)));
        expect(string_out, when(p1, is_equal_to(2)));
        expect(integer_out);
        expect(integer_out);
        expect(string_out, when(p1, is_equal_to(3)));
        expect(integer_out);
----

If this were a real test we could just copy this and paste it in place
of the call to 'cgreen_mocks_are()' and we're done.


BDD Style Cgreen
----------------

As mentioned earlier, *Cgreen* also supports the behaviour driven
style of test driving code. The thinking behind BDD is that we don't
really want to test anything, if we just could specify the behaviour
of our code and ensure that it actually behaves this way we would be
fine.

This might seem like an age old dream, but when you think about it,
there is actually very little difference in the mechanics from
TDD. First we write how we want it, then implement it. But the small
change in wording, from `test´ to `behaviour´, from `test that´ to `ensure
that´, makes a huge difference in thinking, and also very often in quality
of the resulting code.

The SUT - Subject Under Test
~~~~~~~~~~~~~~~~~~~~~~~~~~

Since BDD talks about behaviour, there has to be something that we can
talk about as having the wanted behaviour. This is usually called the
SUT, the Subject Under Test. *Cgreen* in BDD mode requires that you
define a name for it.

[source, c]
-----------------------
#include <cgreen/cgreen.h>
Describe(SUT);
-----------------------

Since *CGreen* supports C++ there you naturally have the objects and
also the Class Under Test. But in plain C you will have to think about
what is actually the "class" under test. E.g. in 'sort.c' you might
see

[source, c]
---------------------
#include <cgreen/cgreen.h>
Describe(Sorter);

Ensure(Sorter, can_sort_an_empty_list) {
  ensure_that(sorter(NULL), is_null);
}
---------------------

In this example you can clearly see what difference BDD style makes
when it comes to naming. Convention, and natural language, dictates
that typical names for what TDD would call tests, now starts with
'can' or 'finds' or other verbs, which makes the specification so much
easier to read.

Yes, I wrote 'specification'. Because that is how BDD views what TDD
basically calls a test suite. The suite specifies the behaviour of a
`class´. (That's why some BDD frameworks draw on 'spec', like
*RSpec*.)

As you can see in the short example above, you have to give the SUT in
the 'Ensure()' declaration too. If you don't, *Cgreen* will get
confused about which style you are using and your source won't
compile.

Contexts and Before and After
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The complete specification of the behaviour of a SUT might become long
and require various forms of setup. When using TDD style you
would probably break this up into multiple suites having their own
'setup()' and 'teardown()'.

With BDD style we could consider the suite as a behaviour
specification for our SUT 'in a particular context'. E.g.

[source, c]
------------------------
#include <cgreen/cgreen.h>

Describe(shopping_basket_for_returning_customer);

Customer *customer;

BeforeEach(shopping_basket_for_returning_customer){
  customer = create_test_customer();
  login(customer);
}

AfterEach(shopping_basket_for_returning_customer) {
  logout(customer);
  destroy_customer(customer);
}

Ensure(shopping_basket_for_returning_customer, can_use_discounts) {
  ...
}
-----------------------------------------------

The ''context'' would then be 'shopping_basket_for_returning_customer',
with the SUT being the shopping basket ''class''.

We have added two ''functions'' which are exactly what they say they
are. Before each ''test'' (or behaviour specification) the customer
record will always be created and the customer logged in, exactly like
we could do with the 'setUp()' function in TDD style. In the same
manner, after each ''test'' the customer will be logged out and the
record destroyed.

I've been holding back on running *Cgreen* with this example because
you need to write all three of the new ''functions'' (of course you
can see that they are CPP macros, right?), to make that work.

And there is one more thing. To run this using the same kind of main
program that we saw earlier, we need to name the SUT or context when
we add the test to a suite:

[source, c]
----------------------

TestSuite *shopping_basket_tests()
{
  TestSuite *suite = create_test_suite();
  add_test_with_context(suite, shopping_basket_for_returning_customer, can_use_discounts);
  return suite;
}
-----------------------

Note that you don't need to (and shouldn't) add any setup or teardown
to the suite. That's already taken care of by the 'BeforeEach()' and
'AfterEach()' definitions.

Now we are ready to run the tests by compiling and linking as
always. And then running the tests through our main program.

[[changing_style, Changing Style]]

Changing Style
~~~~~~~~~~~~~~

If you have some TDD style *Cgreen* test suites, it is quite easy to
change them over to BDD style. Here are the steps required

* Add 'Describe(SUT);'

* Turn your current setup function into a 'BeforeEach()' definition by
changing is signature to match the macro, or simply call the existing
setup function from the BeforeEach(). If you don't have any setup function
you still need to define an empty 'BeforeEach()'.

* Ditto for 'AfterEach()'.

* Add the SUT to each 'Ensure()' as the first parameter.

* Change the call to add the tests to 'add_test_with_context()' by
  adding a third parameter, the name of the SUT.

* Optionally remove the calls to 'set_setup()' and 'set_teardown()'.

Done.

If you want to continue to run the tests using a hand-coded runner,
you can do that by keeping the setup and teardown functions and their
corresponding set_-calls.

It is good that this is a simple process, because you can change over
from TDD style to BDD style in small steps. You can convert one source
file at a time, by just following the recipe above. Everything will
still work as before but your tests and code will likely improve.

And once you have changed style you can fully benefit from the
automatic discovery of tests as described in <<runner, Automatic Test
Discovery>>.


Changing Cgreen Reporting
-------------------------

Replacing the reporter
~~~~~~~~~~~~~~~~~~~~~~
            
In every test suite so far, we have run the tests with this line...

[source,c]
-----------------------
return run_test_suite(our_tests(), create_text_reporter());
-----------------------

We can change the reporting mechanism just by changing this
method.
Here is the code for 'create_text_reporter()'...

[source,c]
-----------------------
TestReporter *create_text_reporter(void) {
    TestReporter *reporter = create_reporter();
    if (reporter == NULL) {
        return NULL;
    }
    reporter->start_suite = &text_reporter_start_suite;
    reporter->start_test = &text_reporter_start_test;
    reporter->show_fail = &show_fail;
    reporter->show_incomplete = &show_incomplete;
    reporter->finish_test = &text_reporter_finish;
    reporter->finish_suite = &text_reporter_finish;
    return reporter;
}
-----------------------

The 'TestReporter' structure contains function pointers that control
the reporting.  When called from 'create_reporter()' constructor,
these pointers are set up with functions that display nothing.  The
text reporter code replaces these with something more dramatic, and
then returns a pointer to this new object.  Thus the
'create_text_reporter()' function effectively extends the object from
'create_reporter()'.
             
            
The text reporter only outputs content at the start of the first test,
at the end of the test run to display the results, when a failure
occurs, and when a test fails to complete.  A quick look at the
+text_reporter.c+ file in *Cgreen* reveals that the overrides just
output a message and chain to the versions in +reporter.h+.


To change the reporting mechanism ourselves, we just have to know a little
about the methods in the 'TestReporter' structure.
             
The TestReporter structure
~~~~~~~~~~~~~~~~~~~~~~~~~~

The *Cgreen* 'TestReporter' is a pseudo class that looks
something like...

[source,c]
-----------------------
typedef struct _TestReporter TestReporter;
struct _TestReporter {
    void (*destroy)(TestReporter *reporter);
    void (*start_suite)(TestReporter *reporter, const char *name, const int count);
    void (*start_test)(TestReporter *reporter, const char *name);
    void (*show_pass)(TestReporter *reporter, const char *file, int line,
                                   const char *message, va_list arguments);
    void (*show_fail)(TestReporter *reporter, const char *file, int line,
                                   const char *message, va_list arguments);
    void (*show_incomplete)(TestReporter *reporter, const char *file, int line,
                                   const char *message, va_list arguments);
    void (*assert_true)(TestReporter *reporter, const char *file, int line, int result,
                                   const char * message, ...);
    void (*finish_test)(TestReporter *reporter, const char *file, int line);
    void (*finish_suite)(TestReporter *reporter, const char *file, int line);
    int passes;
    int failures;
    int exceptions;
    void *breadcrumb;
    int ipc;
    void *memo;
};
-----------------------

The first block are the methods that can be overridden.

'void (*destroy)(TestReporter *reporter)'::
This is the destructor for the default 
structure. If this is overridden, then the overriding function must call 
'destroy_reporter(TestReporter *reporter)' to finish the clean up. 

'void (*start_suite)(TestReporter *reporter, const char *name, const int count)'::
This is the first of the callbacks. At the start of
each test suite *Cgreen* will call this method on the reporter with
the name of the suite being entered and the number of tests in that
suite. The default version keeps track of the stack of tests in the
'breadcrumb' pointer of 'TestReporter'. If you make use of the
breadcrumb functions, as the defaults do, then you will need to call
'reporter_start()' to keep the book keeping in sync.

'void (*start_test)(TestReporter *reporter, const char *name)'::
At the start of each test *Cgreen* will call this method on the
reporter with the name of the test being entered. Again, the default
version keeps track of the stack of tests in the 'breadcrumb' pointer
of 'TestReporter'. If you make use of the breadcrumb functions, as the
defaults do, then you will need to call 'reporter_start()' to keep the
book keeping in sync.

'void (*show_pass)(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments)'::
This method is initially empty as there most reporters see little
point in reporting passing tests (but you might do), so there is no
need to chain the call to any other function. Besides the pointer to
the reporter structure, *Cgreen* also passes the file name of the
test, the line number of failed assertion, the message to show and any
additional parameters to substitute into the message. The message
comes in as 'printf()' style format string, and so the variable
argument list should match the substitutions.

'void (*show_fail)(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments)'::
The partner of 'show_pass()', and the one you'll likely overload first.

'void (*show_incomplete)(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments)'::
When a test fails to complete, this is the handler that is called. As it's an unexpected
outcome, no message is received, but we do get the name of the
test. The text reporter combines this with the breadcrumb to produce
the exception report.

'void (*assert_true)(TestReporter *reporter, const char *file, int line, int result, const char * message, ...)'::
This is not normally
overridden and is really internal. It is the raw entry point for the
test messages from the test suite. By default it dispatches the call
to either 'show_pass()' or 'show_fail()'.

'void (*finish_test)(TestReporter *reporter, const char *file, int line)'::
The counterpart to the '(*start_test)()' call. It is called
on leaving the test. It needs to be chained to the
'reporter_finish()' to keep track of the breadcrumb book keeping.

'void (*finish_suite)(TestReporter *reporter, const char *file, int line)'::
The counterpart to the '(*start_suite)()' call called on
leaving the test suite, and similar to the '(*finish_test)()' if your
reporter needs a handle on that event too. The default text reporter
chains both this and '(*finish_test)()' to the same function where it
figures out if it is the end of the top level suite. If so, it prints
the familiar summary of passes and fails.

The second block is simply resources and book keeping that the reporter
can use to liven up the messages...

[horizontal]
'passes':: The number of passes so far.
'failures'::  The number of failures generated so far.
'exceptions':: The number of test functions that have failed to complete so far. 
'breadcrumb':: This is a pointer to the list of test names in the stack.
				
The 'breadcrumb' pointer is different and needs a little explanation.
Basically it is a stack, analogous to the breadcrumb trail you see on
websites.  Everytime a 'start()' handler is invoked, the name is
placed in this stack.  When a 'finish()' message handler is invoked, a
name is popped off.

There are a bunch of utility functions in +cgreen/breadcrumb.h+ that
can read the state of this stack.  Most useful are
'get_current_from_breadcrumb()' which takes the breadcrumb pointer and
returns the curent test name, and 'get_breadcrumb_depth()' which gives
the current depth of the stack.  A depth of zero means that the test
run has finished.

If you need to traverse all the names in the breadcrumb, then you can
call 'walk_breadcrumb()'.  Here is the full signature...

[source,c]
-----------------------
void walk_breadcrumb(Breadcrumb *breadcrumb, void (*walker)(const char *, void *), void *memo);
-----------------------

The 'void (\*walker)(const char \*, void \*)' is a callback
that will be passed the name of the test suite for each
level of nesting.
It is also passed the 'memo' pointer that was
passed to the 'walk_breadcrumb()' call.
You can use this pointer for anything you want, as
all *Cgreen* does is pass it from call to call.
This is so aggregate information can be kept track of whilst
still being reentrant.


The last parts of the 'TestReporter' structure are...

[horizontal]
'ipc':: This is an internal structure for handling the messaging between reporter
and test suite. You shouldn't touch this.
'memo':: By contrast, this is a spare pointer for your own expansion.
             
An example XML reporter
~~~~~~~~~~~~~~~~~~~~~~~
            
Let's make things real with an example.
Suppose we want to send the output from *Cgreen* in XML format,
say for storing in a repository or for sending across the network.
             
            
Suppose also that we have come up with the following format...

[source,xml]
-----------------------
<?xml?>
<suite name="Top Level">
    <suite name="A Group">
        <test name="a_test_that_passes">
        </test>
        <test name="a_test_that_fails">
            <fail>
                <message>A failure</message>
                <location file="test_as_xml.c" line="8"/>
            </fail>
        </test>
    </suite>
</suite>
-----------------------

In other words a simple nesting of tests with only failures encoded.
The absence of failure is a pass.

Here is a test script, +test_in_xml.c+ that we can use to construct the
above output...

[source,c]
-----------------------
#include <cgreen/cgreen.h>

Ensure(this_test_passes) {
    assert_true(1);
}

Ensure(this_test_fails) {
    assert_true_with_message(0, "A failure");
}

TestSuite *test_group() {
    TestSuite *suite = create_test_suite();
    add_test(suite, this_test_passes);
    add_test(suite, this_test_fails);
    return suite;
}

int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_suite(suite, test_group());
    return run_test_suite(suite, create_text_reporter());
}
-----------------------

The text reporter is used just to confirm that everything is working.
So far it is.

-----------------------
Running "main" (2 tests)...
test_in_xml.c:8: Test Failure: -> test_group -> this_test_fails 
	A failure
Running "main" (2 tests)...
Completed "main": 1 pass, 1 failure, 0 exceptions.
-----------------------
             
          
Our first move is to switch the reporter from text, to our
not yet written XML version...

[source,c]
-----------------------
#include "cgreen/cgreen.h
#include "xml_reporter.h"

...

int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_suite(suite, test_group());
    return run_test_suite(suite, create_xml_reporter());
}
-----------------------

We'll start the ball rolling with the +xml_reporter.h+
header file...

[source,c]
-----------------------
#ifndef _XML_REPORTER_HEADER_
#define _XML_REPORTER_HEADER_

#include "cgreen/reporter.h"

TestReporter *create_xml_reporter();

#endif
-----------------------

...and the simplest possible reporter in +reporter.c+.

[source,c]
-----------------------
#include "xml_reporter.h"
#include "cgreen/reporter.h"

TestReporter *create_xml_reporter() {
    TestReporter *reporter = create_reporter();
    return reporter;
}
-----------------------

One that outputs nothing.

-----------------------
gcc -c test_as_xml.c
gcc -c xml_reporter.c
gcc xml_reporter.o test_as_xml.o -lcgreen -o xml
./xml
-----------------------

Yep, nothing.


Let's add the outer test tags first, so that we can see *Cgreen*
navigating the test suite...

-----------------------
#include "xml_reporter.h"
#include "cgreen/reporter.h"
#include <stdio.h>

static void xml_reporter_start_suite(TestReporter *reporter, const char *name, int count);
static void xml_reporter_start_test(TestReporter *reporter, const char *name);
static void xml_reporter_finish_test(TestReporter *reporter, const char *filename, int line);
static void xml_reporter_finish_suite(TestReporter *reporter, const char *filename, int line);

TestReporter *create_xml_reporter() {
    TestReporter *reporter = create_reporter();
    reporter->start_suite = &xml_reporter_start_suite;
    reporter->start_test = &xml_reporter_start_test;
    reporter->finish_test = &xml_reporter_finish_test;
    reporter->finish_suite = &xml_reporter_finish_suite;
    return reporter;
}

static void xml_reporter_start_suite(TestReporter *reporter, const char *name, int count) {
    printf("<suite name=\"%s\">\n", name);
    reporter_start(reporter, name);
}

static void xml_reporter_start_test(TestReporter *reporter, const char *name) {
    printf("<test name=\"%s\">\n", name);
    reporter_start(reporter, name);
}

static void xml_reporter_finish_test(TestReporter *reporter, const char *filename, int line) {
    reporter_finish(reporter, filename, line);
    printf("</test>\n");
}

static void xml_reporter_finish_suite(TestReporter *reporter, const char *filename, int line) {
    reporter_finish(reporter, filename, line);
    printf("</suite>\n");
}
-----------------------

Although chaining to the underlying 'reporter_start()'
and 'reporter_finish()' functions is optional, I want to
make use of some of the facilities later.

Our output meanwhile, is making it's first tentative steps...

[source,xml]
-----------------------
<suite name="main">
<suite name="test_group">
<test name="this_test_passes">
</test>
<test name="this_test_fails">
</test>
</suite>
</suite>
-----------------------

We don't want a passing message, so the 'show_fail()' function is all we
need...

[source,c]
-----------------------
...
static void xml_show_fail(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments);

TestReporter *create_xml_reporter() {
    TestReporter *reporter = create_reporter();
    reporter->start_suite = &xml_reporter_start_suite;
    reporter->start_test = &xml_reporter_start_test;
    reporter->show_fail = &xml_show_fail;
    reporter->finish_test = &xml_reporter_finish_test;
    reporter->finish_suite = &xml_reporter_finish_suite;
    return reporter;
}

...

static void xml_show_fail(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments) {
    printf("<fail>\n");
    printf("\t<message>\"");
    vprintf(message, arguments);
    printf("\"</message>\n");
    printf("\t<location file=\"%s\" line=\"%d\"/>\n", file, line);
    printf("</fail>\n");
}
-----------------------

We have to use 'vprintf()' to handle the variable argument
list passed to us.
This will probably mean including the +stdarg.h+ header
as well as +stdio.h+.
             
            
This gets us pretty close to what we want...
			
[source,xml]				
-----------------------
<suite name="main">
<suite name="test_group">
<test name="this_test_passes">
</test>
<test name="this_test_fails">
<fail>
        <message>A failure</message>
        <location file="test_in_xml.c" line="9"/>
</fail>
</test>
</suite>
</suite>
-----------------------

For completeness we should add a tag for an incomplete test.
We'll output this as a failure, athough we don't get a location this
time...

[source,c]
-----------------------
#include "xml_reporter.h"
#include "cgreen/reporter.h"
#include "cgreen/breadcrumb.h"

...

static void xml_show_incomplete(TestReporter *reporter, const char *name) {
    printf("<fail>\n");
    printf("\t<message>Failed to complete</message>\n");
    printf("</fail>\n");
}
-----------------------

All that's left then is the XML declaration and the thorny issue of
indenting.  Although the indenting is not strictly necessary, it would
make the output a lot more readable.

The test depth is kept track of for us with the 'breadcrumb' object in
the 'TestReporter' structure.  We'll add an 'indent()' function that
outputs the correct number of tabs...

[source,c]
-----------------------
static indent(TestReporter *reporter) {
    int depth = get_breadcrumb_depth((CgreenBreadcrumb *)reporter->breadcrumb);
    while (depth-- > 0) {
        printf("\t");
    }
}
-----------------------

The 'get_breadcrumb_depth()' function just gives the current test
depth as recorded in the reporters breadcrumb (from
+cgreen/breadcrumb.h+).  As that is just the number of tabs to output,
the implementation is trivial.
            
We can then use this function in the rest of the code.  Here is the
complete listing...

[source,c]
-----------------------
#include "xml_reporter.h"
#include "cgreen/reporter.h"
#include "cgreen/breadcrumb.h"

#include <stdio.h>

static void xml_reporter_start_suite(TestReporter *reporter, const char *name, int count);
static void xml_reporter_start_test(TestReporter *reporter, const char *name);
static void xml_reporter_finish_test(TestReporter *reporter, const char *filename, int line);
static void xml_reporter_finish_suite(TestReporter *reporter, const char *filename, int line);
static void xml_show_fail(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments);

TestReporter *create_xml_reporter() {
    TestReporter *reporter = create_reporter();
    reporter->start_suite = &xml_reporter_start_suite;
    reporter->start_test = &xml_reporter_start_test;
    reporter->show_fail = &xml_show_fail;
    reporter->finish_test = &xml_reporter_finish_test;
    reporter->finish_suite = &xml_reporter_finish_suite;
    return reporter;
}

static indent(TestReporter *reporter) {
    int depth = get_breadcrumb_depth((CgreenBreadcrumb *)reporter->breadcrumb);
    while (depth-- > 0) {
        printf("\t");
    }
}

static void xml_reporter_start_suite(TestReporter *reporter, const char *name, int count) {
    if (get_breadcrumb_depth((CgreenBreadcrumb *)reporter->breadcrumb) == 0) {
        printf("<?xml?>\n");
    }
    indent(reporter);
    printf("<suite name=\"%s\">\n", name);
    reporter_start(reporter, name);
}

static void xml_reporter_start_test(TestReporter *reporter, const char *name) {
    indent(reporter);
    printf("<test name=\"%s\">\n", name);
    reporter_start(reporter, name);
}

static void xml_show_fail(TestReporter *reporter, const char *file, int line, const char *message, va_list arguments) {
    indent(reporter);
    printf("<fail>\n");
    indent(reporter);
    printf("\t<message>");
    vprintf(message, arguments);
    printf("</message>\n");
    indent(reporter);
    printf("\t<location file=\"%s\" line=\"%d\"/>\n", file, line);
    indent(reporter);
    printf("</fail>\n");
}

static void xml_show_incomplete(TestReporter *reporter, const char *name) {
    indent(reporter);
    printf("<fail>\n");
    indent(reporter);
    printf("\t<message>Failed to complete]]></message>\n");
    indent(reporter);
    printf("</fail>\n");
}

static void xml_reporter_finish_test(TestReporter *reporter, const char *filename, int line) {
    reporter_finish(reporter, filename, line);
    indent(reporter);
    printf("</test>\n");
}

static void xml_reporter_finish_suite(TestReporter *reporter, const char *filename, int line) {
    reporter_finish(reporter, filename, line);
    indent(reporter);
    printf("</suite>\n");
}
-----------------------

And finally the desired output...

-----------------------
<?xml?>
<suite name="main">
    <suite name="test_group">
        <test name="this_test_passes">
        </test>
        <test name="this_test_fails">
            <fail>
                <message>A failure</message>
                <location file="test_in_xml.c" line="9"/>
            </fail>
        </test>
    </suite>
</suite>
-----------------------

Job done.

Possible other extensions include reporters that write to 'syslog',
talk to IDE plug-ins, paint pretty printed documents or just return a boolean
for monitoring purposes.

[[runner, Automatic Test Discovery]]

Automatic Test Discovery
------------------------

Forgot to add your test?
~~~~~~~~~~~~~~~~~~~~~~~~

When we write a new test we focus on the details about the test we are
trying to write. And writing tests are no trivial matter so this might
well take a lot of brain power.

So, it comes as no big surprise, that sometimes you write your test
and then forget to add it to the suite. When we run it it appears that it
passed on the first try! Although this *should* really make you
suspicious, sometimes you get so happy that you just continue with
churning out more tests and more code. It's not until some (possibly
looong) time later that you realize, after much headache and
debugging, that the test did not actually pass. It was never even run!

There are practices to minimize the risk of this happening, such as
always running the test as soon as you can set up the test. This way
you will see it fail before trying to get it to pass.

But it is still a practice, something we, as humans, might fail to do
at some point. Usually this happens when we are most stressed and in
need of certainty.

The solution - the 'cgreen-runner'
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Cgreen* gives you a tool to avoid not only the risk of this
happening, but also the extra work and extra code. It is called the
'cgreen-runner'.

The 'cgreen-runner' should come with your *Cgreen* installation if
your platform supports the technique that is required, which is
'programatic access to dynamic loading of libraries'. This means
that a program can load an external library of code into memory and
inspect it. Kind of self-inspection, or reflexion.

So all you have to do is to build a dynamically loadable library of
all tests (and of course your objects under test and other necessary
code). Then you can run the 'cgreen-runner' and point it to the
library. The runner will then load the library, enumerate all tests in
it, and run every test.

It's automatic, and there is nothing to forget.

Using the runner
~~~~~~~~~~~~~~~~

Assuming your tests are in +first_test.c+ the typical command to
build your library using gcc would be

[code, c]
--------------------------
$ gcc -o first_test.o -fPIC first_test.c
$ gcc -shared -o first_test.so first_test.o -lcgreen
--------------------------

The '-fPIC' means to generate +position independent code+ which is
required if you want to load the library dynamically.

How to build a dynamically loadable shared library might vary a lot
depending on your platform. Can't really help you there, sorry!

Now we can run the tests using the 'cgreen-runner':

[code, c]
-------------------------
$ cgreen-runner first_test.dll
Discovered: 2 tests
Opening [first_test.dll] to run all discovered tests ...
Running "main" (2 tests)...
first_test.c:8: Test Failure: -> this_test_should_fail
        Expected [0] to be true
Completed "main": 1 pass, 1 failure, 0 exceptions.
-------------------------

More or less exactly the same output as when we ran our first test in
the beginning of this quickstart tutorial.

Now we can actually delete the main function in our source code. We
don't need all this:

[source, c]
------------------------
int main(int argc, char **argv) {
    TestSuite *suite = create_test_suite();
    add_test(suite, this_test_should_pass);
    add_test(suite, this_test_should_fail);
    return run_test_suite(suite, create_text_reporter());
}
------------------------

It always feel good to delete code, right?

We can also select which test to run:

[code, c]
-------------------------
$ cgreen-runner.exe first_test.dll this_test_should_fail
Discovered: 2 tests
Opening [first_test.dll] to only run test 'this_test_should_fail' ...
Running "main" (2 tests)...
first_test.c:8: Test Failure: -> this_test_should_fail
        [0] should be true
Completed "main": 0 passes, 1 failure, 0 exceptions.
-------------------------

Setup, Teardown and custom reporters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you need setup and teardown functions with the runner you need to
go to the BDD style using 'BeforeEach()' and 'AfterEach()' as
described in <<changing_style, Changing Style>> above.

Neither is it possible to use custom reporters as outlined in the
previous chapter.

In both these cases you need to resort to the standard, programatic,
way of invoking your tests. But, who knows...


[appendix]
GNU Free Documentation License
------------------------------

----

    Version 1.1, March 2000

    
      Copyright (C) 2000  Free Software Foundation, Inc.
59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
Everyone is permitted to copy and distribute verbatim copies
of this license document, but changing it is not allowed.
    
    0. PREAMBLE

    The purpose of this License is to make a manual, textbook,
    or other written document "free" in the sense of freedom: to
    assure everyone the effective freedom to copy and redistribute it,
    with or without modifying it, either commercially or
    noncommercially.  Secondarily, this License preserves for the
    author and publisher a way to get credit for their work, while not
    being considered responsible for modifications made by
    others.

    This License is a kind of "copyleft", which means that
    derivative works of the document must themselves be free in the
    same sense.  It complements the GNU General Public License, which
    is a copyleft license designed for free software.

    We have designed this License in order to use it for manuals
    for free software, because free software needs free documentation:
    a free program should come with manuals providing the same
    freedoms that the software does.  But this License is not limited
    to software manuals; it can be used for any textual work,
    regardless of subject matter or whether it is published as a
    printed book.  We recommend this License principally for works
    whose purpose is instruction or reference.

    1. APPLICABILITY AND DEFINITIONS

    This License applies to any manual or other work that
    contains a notice placed by the copyright holder saying it can be
    distributed under the terms of this License.  The "Document",
    below, refers to any such manual or work.  Any member of the
    public is a licensee, and is addressed as "you".

    A "Modified Version" of the Document means any work
    containing the Document or a portion of it, either copied
    verbatim, or with modifications and/or translated into another
    language.

    A "Secondary Section" is a named appendix or a front-matter
    section of the Document that deals exclusively with the
    relationship of the publishers or authors of the Document to the
    Document's overall subject (or to related matters) and contains
    nothing that could fall directly within that overall subject.
    (For example, if the Document is in part a textbook of
    mathematics, a Secondary Section may not explain any mathematics.)
    The relationship could be a matter of historical connection with
    the subject or with related matters, or of legal, commercial,
    philosophical, ethical or political position regarding
    them.

    The "Invariant Sections" are certain Secondary Sections
    whose titles are designated, as being those of Invariant Sections,
    in the notice that says that the Document is released under this
    License.

    The "Cover Texts" are certain short passages of text that
    are listed, as Front-Cover Texts or Back-Cover Texts, in the
    notice that says that the Document is released under this
    License.

    A "Transparent" copy of the Document means a
    machine-readable copy, represented in a format whose specification
    is available to the general public, whose contents can be viewed
    and edited directly and straightforwardly with generic text
    editors or (for images composed of pixels) generic paint programs
    or (for drawings) some widely available drawing editor, and that
    is suitable for input to text formatters or for automatic
    translation to a variety of formats suitable for input to text
    formatters.  A copy made in an otherwise Transparent file format
    whose markup has been designed to thwart or discourage subsequent
    modification by readers is not Transparent.  A copy that is not
    "Transparent" is called "Opaque".

    Examples of suitable formats for Transparent copies include
    plain ASCII without markup, Texinfo input format, LaTeX input
    format, SGML or XML using a publicly available DTD, and
    standard-conforming simple HTML designed for human modification.
    Opaque formats include PostScript, PDF, proprietary formats that
    can be read and edited only by proprietary word processors, SGML
    or XML for which the DTD and/or processing tools are not generally
    available, and the machine-generated HTML produced by some word
    processors for output purposes only.

    The "Title Page" means, for a printed book, the title page
    itself, plus such following pages as are needed to hold, legibly,
    the material this License requires to appear in the title page.
    For works in formats which do not have any title page as such,
    "Title Page" means the text near the most prominent appearance of
    the work's title, preceding the beginning of the body of the
    text.

    2. VERBATIM COPYING

    You may copy and distribute the Document in any medium,
    either commercially or noncommercially, provided that this
    License, the copyright notices, and the license notice saying this
    License applies to the Document are reproduced in all copies, and
    that you add no other conditions whatsoever to those of this
    License.  You may not use technical measures to obstruct or
    control the reading or further copying of the copies you make or
    distribute.  However, you may accept compensation in exchange for
    copies.  If you distribute a large enough number of copies you
    must also follow the conditions in section 3.

    You may also lend copies, under the same conditions stated
    above, and you may publicly display copies.

    3. COPYING IN QUANTITY

    If you publish printed copies of the Document numbering more
    than 100, and the Document's license notice requires Cover Texts,
    you must enclose the copies in covers that carry, clearly and
    legibly, all these Cover Texts: Front-Cover Texts on the front
    cover, and Back-Cover Texts on the back cover.  Both covers must
    also clearly and legibly identify you as the publisher of these
    copies.  The front cover must present the full title with all
    words of the title equally prominent and visible.  You may add
    other material on the covers in addition.  Copying with changes
    limited to the covers, as long as they preserve the title of the
    Document and satisfy these conditions, can be treated as verbatim
    copying in other respects.

    If the required texts for either cover are too voluminous to
    fit legibly, you should put the first ones listed (as many as fit
    reasonably) on the actual cover, and continue the rest onto
    adjacent pages.

    If you publish or distribute Opaque copies of the Document
    numbering more than 100, you must either include a
    machine-readable Transparent copy along with each Opaque copy, or
    state in or with each Opaque copy a publicly-accessible
    computer-network location containing a complete Transparent copy
    of the Document, free of added material, which the general
    network-using public has access to download anonymously at no
    charge using public-standard network protocols.  If you use the
    latter option, you must take reasonably prudent steps, when you
    begin distribution of Opaque copies in quantity, to ensure that
    this Transparent copy will remain thus accessible at the stated
    location until at least one year after the last time you
    distribute an Opaque copy (directly or through your agents or
    retailers) of that edition to the public.

    It is requested, but not required, that you contact the
    authors of the Document well before redistributing any large
    number of copies, to give them a chance to provide you with an
    updated version of the Document.

    4. MODIFICATIONS

    You may copy and distribute a Modified Version of the
    Document under the conditions of sections 2 and 3 above, provided
    that you release the Modified Version under precisely this
    License, with the Modified Version filling the role of the
    Document, thus licensing distribution and modification of the
    Modified Version to whoever possesses a copy of it.  In addition,
    you must do these things in the Modified Version:

      Use in the Title Page
      (and on the covers, if any) a title distinct from that of the
      Document, and from those of previous versions (which should, if
      there were any, be listed in the History section of the
      Document).  You may use the same title as a previous version if
      the original publisher of that version gives permission.
      

      List on the Title Page,
      as authors, one or more persons or entities responsible for
      authorship of the modifications in the Modified Version,
      together with at least five of the principal authors of the
      Document (all of its principal authors, if it has less than
      five).
      

      State on the Title page
      the name of the publisher of the Modified Version, as the
      publisher.
      

      Preserve all the
      copyright notices of the Document.
      

      Add an appropriate
      copyright notice for your modifications adjacent to the other
      copyright notices.
      

      Include, immediately
      after the copyright notices, a license notice giving the public
      permission to use the Modified Version under the terms of this
      License, in the form shown in the Addendum below.
      

      Preserve in that license
      notice the full lists of Invariant Sections and required Cover
      Texts given in the Document's license notice.
      

      Include an unaltered
      copy of this License.
      

      Preserve the section
      entitled "History", and its title, and add to it an item stating
      at least the title, year, new authors, and publisher of the
      Modified Version as given on the Title Page.  If there is no
      section entitled "History" in the Document, create one stating
      the title, year, authors, and publisher of the Document as given
      on its Title Page, then add an item describing the Modified
      Version as stated in the previous sentence.
      

      Preserve the network
      location, if any, given in the Document for public access to a
      Transparent copy of the Document, and likewise the network
      locations given in the Document for previous versions it was
      based on.  These may be placed in the "History" section.  You
      may omit a network location for a work that was published at
      least four years before the Document itself, or if the original
      publisher of the version it refers to gives permission.
      

      In any section entitled
      "Acknowledgements" or "Dedications", preserve the section's
      title, and preserve in the section all the substance and tone of
      each of the contributor acknowledgements and/or dedications
      given therein.
      

      Preserve all the
      Invariant Sections of the Document, unaltered in their text and
      in their titles.  Section numbers or the equivalent are not
      considered part of the section titles.
      

      Delete any section
      entitled "Endorsements".  Such a section may not be included in
      the Modified Version.
      

      Do not retitle any
      existing section as "Endorsements" or to conflict in title with
      any Invariant Section.
      
    
    If the Modified Version includes new front-matter sections
    or appendices that qualify as Secondary Sections and contain no
    material copied from the Document, you may at your option
    designate some or all of these sections as invariant.  To do this,
    add their titles to the list of Invariant Sections in the Modified
    Version's license notice.  These titles must be distinct from any
    other section titles.

    You may add a section entitled "Endorsements", provided it
    contains nothing but endorsements of your Modified Version by
    various parties--for example, statements of peer review or that
    the text has been approved by an organization as the authoritative
    definition of a standard.

    You may add a passage of up to five words as a Front-Cover
    Text, and a passage of up to 25 words as a Back-Cover Text, to the
    end of the list of Cover Texts in the Modified Version.  Only one
    passage of Front-Cover Text and one of Back-Cover Text may be
    added by (or through arrangements made by) any one entity.  If the
    Document already includes a cover text for the same cover,
    previously added by you or by arrangement made by the same entity
    you are acting on behalf of, you may not add another; but you may
    replace the old one, on explicit permission from the previous
    publisher that added the old one.

    The author(s) and publisher(s) of the Document do not by
    this License give permission to use their names for publicity for
    or to assert or imply endorsement of any Modified Version.

    5. COMBINING DOCUMENTS

    You may combine the Document with other documents released
    under this License, under the terms defined in section 4 above for
    modified versions, provided that you include in the combination
    all of the Invariant Sections of all of the original documents,
    unmodified, and list them all as Invariant Sections of your
    combined work in its license notice.

    The combined work need only contain one copy of this
    License, and multiple identical Invariant Sections may be replaced
    with a single copy.  If there are multiple Invariant Sections with
    the same name but different contents, make the title of each such
    section unique by adding at the end of it, in parentheses, the
    name of the original author or publisher of that section if known,
    or else a unique number.  Make the same adjustment to the section
    titles in the list of Invariant Sections in the license notice of
    the combined work.

    In the combination, you must combine any sections entitled
    "History" in the various original documents, forming one section
    entitled "History"; likewise combine any sections entitled
    "Acknowledgements", and any sections entitled "Dedications".  You
    must delete all sections entitled "Endorsements."

    6. COLLECTIONS OF DOCUMENTS

    You may make a collection consisting of the Document and
    other documents released under this License, and replace the
    individual copies of this License in the various documents with a
    single copy that is included in the collection, provided that you
    follow the rules of this License for verbatim copying of each of
    the documents in all other respects.

    You may extract a single document from such a collection,
    and distribute it individually under this License, provided you
    insert a copy of this License into the extracted document, and
    follow this License in all other respects regarding verbatim
    copying of that document.

    7. AGGREGATION WITH INDEPENDENT WORKS
    
    A compilation of the Document or its derivatives with other
    separate and independent documents or works, in or on a volume of
    a storage or distribution medium, does not as a whole count as a
    Modified Version of the Document, provided no compilation
    copyright is claimed for the compilation.  Such a compilation is
    called an "aggregate", and this License does not apply to the
    other self-contained works thus compiled with the Document, on
    account of their being thus compiled, if they are not themselves
    derivative works of the Document.

    If the Cover Text requirement of section 3 is applicable to
    these copies of the Document, then if the Document is less than
    one quarter of the entire aggregate, the Document's Cover Texts
    may be placed on covers that surround only the Document within the
    aggregate.  Otherwise they must appear on covers around the whole
    aggregate.

    8. TRANSLATION

    Translation is considered a kind of modification, so you may
    distribute translations of the Document under the terms of section
    4.  Replacing Invariant Sections with translations requires
    special permission from their copyright holders, but you may
    include translations of some or all Invariant Sections in addition
    to the original versions of these Invariant Sections.  You may
    include a translation of this License provided that you also
    include the original English version of this License.  In case of
    a disagreement between the translation and the original English
    version of this License, the original English version will
    prevail.

    9. TERMINATION
    
    You may not copy, modify, sublicense, or distribute the
    Document except as expressly provided for under this License.  Any
    other attempt to copy, modify, sublicense or distribute the
    Document is void, and will automatically terminate your rights
    under this License.  However, parties who have received copies, or
    rights, from you under this License will not have their licenses
    terminated so long as such parties remain in full
    compliance.

    10. FUTURE REVISIONS OF THIS LICENSE

    The Free Software Foundation may publish new, revised
    versions of the GNU Free Documentation License from time to time.
    Such new versions will be similar in spirit to the present
    version, but may differ in detail to address new problems or
    concerns.  See http://www.gnu.org/copyleft/.

    Each version of the License is given a distinguishing
    version number.  If the Document specifies that a particular
    numbered version of this License "or any later version" applies to
    it, you have the option of following the terms and conditions
    either of that specified version or of any later version that has
    been published (not as a draft) by the Free Software Foundation.
    If the Document does not specify a version number of this License,
    you may choose any version ever published (not as a draft) by the
    Free Software Foundation.

    How to use this License for your documents

    To use this License in a document you have written, include
    a copy of the License in the document and put the following
    copyright and license notices just after the title page:


      Copyright (c)  YEAR  YOUR NAME.
      Permission is granted to copy, distribute and/or modify this document
      under the terms of the GNU Free Documentation License, Version 1.1
      or any later version published by the Free Software Foundation;
      with the Invariant Sections being LIST THEIR TITLES, with the
      Front-Cover Texts being LIST, and with the Back-Cover Texts being LIST.
      A copy of the license is included in the section entitled "GNU
      Free Documentation License".


    If you have no Invariant Sections, write "with no Invariant
    Sections" instead of saying which ones are invariant.  If you have
    no Front-Cover Texts, write "no Front-Cover Texts" instead of
    "Front-Cover Texts being LIST"; likewise for Back-Cover
    Texts.

    If your document contains nontrivial examples of program
    code, we recommend releasing these examples in parallel under your
    choice of free software license, such as the GNU General Public
    License, to permit their use in free software.

----
